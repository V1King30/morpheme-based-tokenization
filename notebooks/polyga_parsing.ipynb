{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../files/words_data.txt', 'r', encoding='utf-8') as f:\n",
    "    words = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = words.split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'́'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emphasis_symbol = words_list[1][5]\n",
    "words_list[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(word_record):\n",
    "    # replace emphasis and pseudo 'i' symbol\n",
    "    word_record = word_record \\\n",
    "        .replace(emphasis_symbol, '') \\\n",
    "        .replace('[', '') \\\n",
    "        .replace(']', '') \\\n",
    "        .replace('í', 'і') \\\n",
    "        .replace('á', 'а') \\\n",
    "        .replace('é', 'е') \\\n",
    "        .replace('ý', 'у') \\\n",
    "        .replace('ó', 'о') \\\n",
    "        .replace('й/і', 'ї') \\\n",
    "        .replace(\"'/а\", '/я') \\\n",
    "        .replace(\"'/у\", '/ю') \\\n",
    "        .replace(\"й/а\", 'я') \\\n",
    "        .replace(\"й/у\", 'ю') \\\n",
    "    # cut whitespaces\n",
    "    word_record = word_record.strip()\n",
    "    return word_record\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_list = [process_record(word) for word in words_list if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/polyga_clean_word_data.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(clean_words_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing word forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_forms_pattern = r'\\(~(.*?)\\)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_forms(word_record):\n",
    "    word_forms_findings = re.findall(word_forms_pattern, word_record)\n",
    "    flattened_forms_list = []\n",
    "    for word_form in word_forms_findings:\n",
    "        flattened_forms_list.extend(word_form.split(', ~'))\n",
    "    word = word_record.split(' ')[0]\n",
    "    return (word, flattened_forms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_forms = [extract_word_forms(word_record) for word_record in clean_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('абажур/н/ий', []),\n",
       " ('абажур/чик', []),\n",
       " ('абат/ис/а', []),\n",
       " ('абат/ств/о', []),\n",
       " ('абетк/а', ['тц/і', 'ток']),\n",
       " ('абетк/ов/ий', []),\n",
       " ('абіссин/ськ/ий', []),\n",
       " ('абіурієнт/к/а', ['т/ц/і', 'т/ок']),\n",
       " ('аблакт/ува/ти', []),\n",
       " ('аблакц/і/он/ізм', [])]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_with_forms[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_occurence(text, target_symbol):\n",
    "    for indx, symbol in enumerate(reversed(text)):\n",
    "        if symbol == target_symbol:\n",
    "            return len(text) - 1 - indx\n",
    "    return -1\n",
    "\n",
    "def explode_word_forms(word_with_forms):\n",
    "    word, forms = word_with_forms\n",
    "    constructed_words = [word]\n",
    "    for form in forms:\n",
    "        form_first_symbol = form[0]\n",
    "        word_last_indx = find_last_occurence(word, form_first_symbol)\n",
    "        constructed_words.append(word[:word_last_indx] + form + '\\t ---------')\n",
    "    return constructed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('абетк/а', ['тц/і', 'ток'])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word, forms = words_with_forms[4]\n",
    "word, forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exploded_words = []\n",
    "\n",
    "for processed_word in words_with_forms:\n",
    "    exploded_words = explode_word_forms(processed_word)\n",
    "    all_exploded_words.extend(exploded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45108"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_exploded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/polyga_clean_word_data.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(clean_words_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'YOUR_API_KEY'\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I need to encode words forms from intup and get all forms as a list. \\n I have an input of a word with proposed additional forms. Here is an example: 'абон/ент/к/а (т/ц/і, т/ок)'. I need to convert such input to a list of corresponding words, in this case 'абон/ент/к/а, абон/ент/ц/і, абон/ент/ок'. I need only words separated by comma as output. Now encode this for me - {input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt_to_gpt(prompt, model=\"gpt-3.5-turbo\", temperature=0.1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "\n",
    "for word, forms in tqdm(words_with_forms):\n",
    "    if len(forms) == 0:\n",
    "        all_words.append(word)\n",
    "    else:\n",
    "        formatted_input = f\"{word} ({', '.join(forms)})\"\n",
    "        response = send_prompt_to_gpt(prompt.format(input=formatted_input))\n",
    "        all_words.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/polyga_clean_word_data_ai.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(all_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academic_writings_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8 (tags/v3.12.8:2dc476b, Dec  3 2024, 19:30:04) [MSC v.1942 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abdd82aec9bcd2c3745ccf49686a3b0ec82fa4e32b02eeaa725aa3e53c148f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
